{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import sample as rsample\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "GRID_SIZE = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### State is the current image of the game, simplified here as a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def episode():\n",
    "    \"\"\" \n",
    "    Coroutine function for an episode.     \n",
    "        Action has to be explicitly sent (via \"send\") to this co-routine.\n",
    "    \"\"\"\n",
    "    x, y, x_basket = (\n",
    "        np.random.randint(0, GRID_SIZE),        # X of fruit\n",
    "        0,                                      # Y of dot\n",
    "        np.random.randint(1, GRID_SIZE - 1))    # X of basket\n",
    "        \n",
    "    while True:\n",
    "        # Reset grid\n",
    "        X = np.zeros((GRID_SIZE, GRID_SIZE))  \n",
    "        # Draw the fruit in the screen\n",
    "        X[y, x] = 1.\n",
    "        # Draw the basket\n",
    "        bar = range(x_basket - 1, x_basket + 2)\n",
    "        X[-1, bar] = 1.\n",
    "        \n",
    "        # End of game is known when fruit is at penultimate line of grid.\n",
    "        # End represents either the reward (a win or a loss)\n",
    "        end = int(y >= GRID_SIZE - 2)\n",
    "        if end and x not in bar:\n",
    "            end *= -1\n",
    "            \n",
    "        action = yield X[np.newaxis], end    \n",
    "        if end:\n",
    "            break\n",
    "\n",
    "        x_basket = min(max(x_basket + action, 1), GRID_SIZE - 2)\n",
    "        y += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experience_replay(batch_size): ????????????????????????\n",
    "    \"\"\"\n",
    "    Coroutine function for implementing experience replay.    \n",
    "        Provides a new experience by calling \"send\", which in turn yields \n",
    "        a random batch of previous replay experiences.\n",
    "    \"\"\"\n",
    "    memory = []\n",
    "    while True:\n",
    "        # experience is a tuple containing (S, action, reward, S_prime)\n",
    "        experience = yield rsample(memory, batch_size) if batch_size <= len(memory) else None\n",
    "        memory.append(experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_img():\n",
    "    \"\"\"\n",
    "    Coroutine to store images in the \"images\" directory\n",
    "    \"\"\"\n",
    "    if 'images' not in os.listdir('.'):\n",
    "        os.mkdir('images')\n",
    "    frame = 0\n",
    "    while True:\n",
    "        screen = (yield)\n",
    "        plt.imshow(screen[0], interpolation='none')\n",
    "        plt.savefig('`images/%03i.png' % frame)\n",
    "        frame += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modelBuild():\n",
    "    nb_epochs = 500\n",
    "    batch_size = 128\n",
    "    epsilon = .8\n",
    "    gamma = .8\n",
    "\n",
    "    # Recipe of deep reinforcement learning model\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(16, nb_row=3, nb_col=3, input_shape=(1, GRID_SIZE, GRID_SIZE), activation='relu'))\n",
    "    model.add(Convolution2D(16, nb_row=3, nb_col=3, activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(3))\n",
    "    model.compile(RMSprop(), 'MSE')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual code that implements the deep Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_replay = experience_replay(batch_size)\n",
    "exp_replay.next()  # Start experience-replay coroutine\n",
    "\n",
    "for i in xrange(nb_epochs):\n",
    "    ep = episode()\n",
    "    S, reward = ep.next()  # Start coroutine of single entire episode\n",
    "    loss = 0.\n",
    "    try:\n",
    "        while True:\n",
    "            action = np.random.randint(-1, 2) \n",
    "            if np.random.random() > epsilon:\n",
    "                # Get the index of the maximum q-value of the model.\n",
    "                # Subtract one because actions are either -1, 0, or 1\n",
    "                action = np.argmax(model.predict(S[np.newaxis]), axis=-1)[0] - 1\n",
    "\n",
    "            S_prime, reward = ep.send(action)\n",
    "            experience = (S, action, reward, S_prime)\n",
    "            S = S_prime\n",
    "            \n",
    "            batch = exp_replay.send(experience)\n",
    "            if batch:\n",
    "                inputs = []\n",
    "                targets = []\n",
    "                for s, a, r, s_prime in batch:\n",
    "                    # The targets of unchosen actions are the q-values of the model,\n",
    "                    # so that the corresponding errors are 0. The targets of chosen actions\n",
    "                    # are either the rewards, in case a terminal state has been reached, \n",
    "                    # or future discounted q-values, in case episodes are still running.\n",
    "                    t = model.predict(s[np.newaxis]).flatten()\n",
    "                    t[a + 1] = r\n",
    "                    # 这种情况是 fruit 还没到底部, 还不能判断是否成功\n",
    "                    if not r: \n",
    "                        t[a + 1] = r + gamma * model.predict(s_prime[np.newaxis]).max(axis=-1)\n",
    "                    targets.append(t)\n",
    "                    inputs.append(s)\n",
    "                \n",
    "                loss += model.train_on_batch(np.array(inputs), np.array(targets))\n",
    "\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    \n",
    "    if (i + 1) % 100 == 0:\n",
    "        print 'Epoch %i, loss: %.6f' % (i + 1, loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
